\doxysection{DCL\+::CNeural\+Network\+Feed\+Forward Class Reference}
\hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward}{}\label{class_d_c_l_1_1_c_neural_network_feed_forward}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}


This is a "{}feed forward"{} neural network.  




{\ttfamily \#include $<$Neural\+Net\+Feed\+Forward.\+h$>$}

\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_a7a637bc9b034c071daab0e917bbd18a2}{CNeural\+Network\+Feed\+Forward}} ()
\begin{DoxyCompactList}\small\item\em Constructor. \end{DoxyCompactList}\item 
\mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_a9beb7ce0130efd7b63a802a255003856}{CNeural\+Network\+Feed\+Forward}} (int i\+Num\+Inputs, int i\+Num\+Outputs, int i\+Num\+Layers, int i\+Num\+Neurons\+Per\+Layer)
\begin{DoxyCompactList}\small\item\em Constructor. \end{DoxyCompactList}\item 
void \mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_acb77c7337cf1440793b45b312aa2dac4}{create}} (int i\+Num\+Inputs, int i\+Num\+Outputs, int i\+Num\+Layers, int i\+Num\+Neurons\+Per\+Layer)
\begin{DoxyCompactList}\small\item\em Creates/recreates the network. \end{DoxyCompactList}\item 
std\+::vector$<$ double $>$ \mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_a9082844ff1fbbb3890d468c4acf3bd15}{get\+Neuron\+Weights}} (void) const
\begin{DoxyCompactList}\small\item\em Returns a vector holding all the weights of all the neurons in each of the layers, including the output layer. \end{DoxyCompactList}\item 
int \mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_af9100130b7f3c95cb02d9d59515388fa}{get\+Number\+Of\+Weights}} (void) const
\begin{DoxyCompactList}\small\item\em Returns the number of weights of all the neurons in each of the layers, including the output layer. \end{DoxyCompactList}\item 
void \mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_a5c907e87498ba77d90844fce32e4375d}{replace\+Weights}} (const std\+::vector$<$ double $>$ \&new\+Weights)
\begin{DoxyCompactList}\small\item\em Replaces the weights of all the neurons in each of the layers with the ones given. \end{DoxyCompactList}\item 
std\+::vector$<$ double $>$ \mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_a703b5d1758980e9365bd3be29525dd68}{update}} (std\+::vector$<$ double $>$ \&vec\+Inputs)
\begin{DoxyCompactList}\small\item\em Given a vector of inputs, updates the neural network and returns the output values. \end{DoxyCompactList}\item 
void \mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_a28acb02d9185de77b5f480ac074622f2}{set\+Weight\+Bias}} (double d\+Weight\+Bias=-\/1)
\begin{DoxyCompactList}\small\item\em Sets the bias which is used during update when computing the output for a neuron (weight \texorpdfstring{$\ast$}{*} input) \texorpdfstring{$\ast$}{*} bias. \end{DoxyCompactList}\item 
void \mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_a22e7d0f6ac35cad6149524a4db5971f1}{set\+Sigmoid\+Response}} (double d\+Response=1)
\begin{DoxyCompactList}\small\item\em Sets the response value used by the sigmoid function which sets the shape of the curve produced from the sigmoid function. \end{DoxyCompactList}\item 
std\+::vector$<$ int $>$ \mbox{\hyperlink{class_d_c_l_1_1_c_neural_network_feed_forward_a7bbc7c1e261995931e9076f06a1d0a81}{calculate\+Split\+Points}} (void)
\begin{DoxyCompactList}\small\item\em Calculate splits points used by the genetic algorithm training class. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
This is a "{}feed forward"{} neural network. 

A feed forward neural network is one whereby the inputs go into the network and move along in one direction towards the outputs. The advantages of this is speed. The disadvantages is that it is not as complex. A neural network, regardless of it\textquotesingle{}s type, simulates connections and firings between neurons in a biological brain.

Here\textquotesingle{}s a short list of animals and the approximate number of neurons within their minds... Killer whale\+: 43 100 000 000 Dolphin\+: 18 750 000 000 Human\+: 16 340 000 000 Bottlenose dolphin\+: 12 700 000 000 Gorilla (Western)\+: 9 100 000 000 Asian elephant\+: 6 775 000 000 Red and green macaw\+: 2 646 000 000 Giraffe\+: 1 731 000 000 Snowy old\+: 1 270 000 000 German shepherd\+: 885 460 000 Lion\+: 545 240 000 Pig\+: 425 000 000 Brown bear\+: 250 970 000 House cat\+: 249 830 000 Mallard duck\+: 112 255 000 Common wood pigeon\+: 51 325 000 Brown rat\+: 31 000 000 Golden hamster\+: 17 000 000 Bat\+: 6 000 000 Ant\+: 250 000 Cockroach\+: 200 000 Honey bee\+: 170 000 Cricket\+: 50 000 Jelly fish\+: 5 600 Common fruit fly\+: 2 500 Starfish\+: 500 Tardigrade\+: 200 Sea sponge\+: 0

We won\textquotesingle{}t be simulating the mind of a killer whale any time soon, but the above list gives us an insight into how complex these creatures\textquotesingle{} minds are.

In a human mind, each neuron is connected via its dendrites to approximately 10,000 other neurons. This means that it\textquotesingle{}s possible to have 1,000,000,000,000,000 connections!

A brain’s neurons either fire or they don’t. The strength of the emitted signal does not vary, only the frequency. The neuron sums all the incoming signals from the synapses and if the total signal exceeds a threshold value, the neuron fires and an electrical signal is sent shooting down the axon. Also, each neuron operates at around 100Hz.

Obviously, we can\textquotesingle{}t use millions of neurons, but with a number as small as just ten, we are able to get some incredibly interesting behaviour.

The great thing about all these neurons is that they are great for generalizing. and if trained well, can exhibit fantastic creature like behaviour. Neural networks are commonly used for pattern recognition. This is because they are great at mapping an input state (the pattern it’s trying to recognize) to an output state (the pattern it has been trained to recognize)

Let’s take the example of character recognition. Imagine an LED panel made up of a grid of lights 8×8. Each light can be on or off, so the panel can be used to display the numbers 0 through to 9. To solve the problem, a network must be designed which will accept the state of the panel as an input and then output either a one or a zero. A one to indicate that, for example, the character 7 is being displayed and zero if it thinks it is not. The neural net will have 64 inputs, a layer of neurons, all feeding their output into just one neuron in the output layer. Once the neural network has been setup with the inputs, layers and output, it must be trained to recognize the character 7. One way of doing this is to initialize the neural net with random weights and then feed it a series of inputs which represent the different characters shown on the panel. For each character, we check to see what it\textquotesingle{}s output is and adjust the weights accordingly. If the input pattern we feed it is not a 7, then we know the neural network should output a zero. So for every non 7 character, the weights are adjusted slightly so the output tends toward zero. When it’s presented with a pattern that represents the character 7, the weights are adjusted so the output tends toward the number one.

We could increase the number of outputs to ten. Then it would be possible to train the network to recognize all the digits 0 through 9. But let\textquotesingle{}s go even more nuts! Let’s increase the outputs so the entire alphabet can be recognized. This is how handwriting recognition works. For each character, the network is trained to recognize many different versions of a letter. Eventually the network will not only be able to recognize the letters it has been trained with, but it will also be able to generalize. That is, if a letter is drawn slightly differently from the letters used during training, the network will stand a pretty good chance of recognizing it.

Not only can a network be used for recognition of characters, but any type of image. People\textquotesingle{}s faces, photos of animals or objects. A network can also even be used for horse racing prediction and, interesting to us, game object navigation (flocking, avoidance, moving towards food, away from enemies) and more!

This type of network training is called "{}\+Supervised training"{} and the data used to train it is called a "{}training set"{}.

Now let\textquotesingle{}s get back to programming... We simulate all these neurons and their connections (On a much smaller scale) within this class. Each neuron can have inputs which are either on or off, 0 or 1 and they are scaled by a weight. Each of these weights are summed by their connected neuron and if the sum is over a certain threshold, the neuron fires. Just like a biological neuron, kinda\+:) The inputs can be positive or negative so can contribute towards or against a neuron firing. A feed forward network has it\textquotesingle{}s inputs at one end and then one or more layers of neurons, with the initial layer taking and summing the inputs, firing or not and then sending that onto the next layer of neurons until the final layer is reached. Each neuron in each layer is connected to each and every neuron in the next layer. The outputs of the final neuron layer are then retrieved by use to interprete however we choose.

The outputs of a network are either 0 or 1, however, we can modify them so that the output is S shaped, with the values being around 0.\+5. To do this, we use a sigma function, sigma being a Greek word for something which is S shaped. We can then use these modified output values as anything we like, for example, the amount of rotation around an axis, or velocity increase/decrease, or whether to fire a weapon and more.

When using the sigmoid method, it requires a value named response. This modifies the curve of the output value and is typically set to 1. Larger values produce a smoother curve where the values tend towards a straight line, whereas smaller values produce a tighter curve where the value centres more towards 0.\+5.

When choosing which inputs to use, the fewer the better because the network won\textquotesingle{}t have to "{}work as hard"{} to find a relationship between them all. So for example, if we\textquotesingle{}re trying to make some game entity move towards certain objects, instead of specifying lots of inputs such as entity positions(x and y), vectors towards the objects (also x,y) and the entity\textquotesingle{}s direction vector (another x and y), we could more efficiently give the network just a single float for it\textquotesingle{}s rotation and a single float which represents the angle between the rotation of the entity and the object instead. These fewer inputs still represent all the information needed, but as there are less of them, the network will be able to find the relationship between them easier. And with this, the network can contain fewer neurons and be faster to compute as a result.

Inputs should each be given, which are around the same scale, for example if a vector was given to a target and it wasn\textquotesingle{}t normalized, and another input was given, perhaps say the entity\textquotesingle{}s normalized direction vector, the vector with the larger magnitude would have a much greater impact on the neural network than the entity\textquotesingle{}s direction vector and as a result, the network may have a really hard time trying to find a relationship between them. Another great tip is that sometimes, centering the inputs around zero give better results.

What about the number of neurons and number of layers? What should we choose there? It\textquotesingle{}s pretty much down to trial and error and getting a feel for how the network is behaving. Typically, one layer is enough. As for the number of neurons, yup, same again, trial and error. However, too few and the network won\textquotesingle{}t stand a chance at giving good outputs and too many, well, it\textquotesingle{}ll be OK, but it will be slower to compute.

So with all that out of the way, how do we actually use them? We create a network, choosing the number of inputs we\textquotesingle{}ll give it, the number of layers, the number of neurons in each layer and the number of outputs. We then use another class, to train the network. Then once the we\textquotesingle{}ve found networks which perform as we wish, we can then save those best performing networks to a file. Then in our game/program, we can create a network, for each entity in our program, load the networks weights and configuration from the file and then update it, passing it it\textquotesingle{}s input we used during training and getting the outputs and applying those the same we did during training, nice.

At the moment, the training class we use is in \doxylink{_genetic_algorithm_8h}{genetic\+Algorithm.\+h}/cpp, go there to see more detail on how we train the networks. 

Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8h_source_l00199}{199}} of file \mbox{\hyperlink{_neural_net_feed_forward_8h_source}{Neural\+Net\+Feed\+Forward.\+h}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_a7a637bc9b034c071daab0e917bbd18a2}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!CNeuralNetworkFeedForward@{CNeuralNetworkFeedForward}}
\index{CNeuralNetworkFeedForward@{CNeuralNetworkFeedForward}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{CNeuralNetworkFeedForward()}{CNeuralNetworkFeedForward()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_a7a637bc9b034c071daab0e917bbd18a2} 
DCL\+::\+CNeural\+Network\+Feed\+Forward\+::\+CNeural\+Network\+Feed\+Forward (\begin{DoxyParamCaption}{}{}\end{DoxyParamCaption})}



Constructor. 

Sets to default values. 

Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00027}{27}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.

\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_a9beb7ce0130efd7b63a802a255003856}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!CNeuralNetworkFeedForward@{CNeuralNetworkFeedForward}}
\index{CNeuralNetworkFeedForward@{CNeuralNetworkFeedForward}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{CNeuralNetworkFeedForward()}{CNeuralNetworkFeedForward()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_a9beb7ce0130efd7b63a802a255003856} 
DCL\+::\+CNeural\+Network\+Feed\+Forward\+::\+CNeural\+Network\+Feed\+Forward (\begin{DoxyParamCaption}\item[{int}]{i\+Num\+Inputs}{, }\item[{int}]{i\+Num\+Outputs}{, }\item[{int}]{i\+Num\+Layers}{, }\item[{int}]{i\+Num\+Neurons\+Per\+Layer}{}\end{DoxyParamCaption})}



Constructor. 


\begin{DoxyParams}{Parameters}
{\em i\+Num\+Inputs} & The number of inputs this network will have. \\
\hline
{\em i\+Num\+Outputs} & The number of outputs this network will have. \\
\hline
{\em i\+Num\+Layers} & The number of neuron layers this network will have. \\
\hline
{\em i\+Num\+Neurons\+Per\+Layer} & The number of neurons per layer this network will have.\\
\hline
\end{DoxyParams}
Sets values to the ones given 

Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00032}{32}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.



\doxysubsection{Member Function Documentation}
\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_a7bbc7c1e261995931e9076f06a1d0a81}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!calculateSplitPoints@{calculateSplitPoints}}
\index{calculateSplitPoints@{calculateSplitPoints}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{calculateSplitPoints()}{calculateSplitPoints()}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_a7bbc7c1e261995931e9076f06a1d0a81} 
std\+::vector$<$ int $>$ DCL\+::\+CNeural\+Network\+Feed\+Forward\+::calculate\+Split\+Points (\begin{DoxyParamCaption}\item[{void}]{}{}\end{DoxyParamCaption})}



Calculate splits points used by the genetic algorithm training class. 

\begin{DoxyReturn}{Returns}
A vector of ints holding the split points between individual neurons.
\end{DoxyReturn}
This returns points in the weights vector which are spaced out between each neuron. This makes it so that when we split the weights of the parents, when creating child networks, the splits are between neurons, rather than ANY position which could be between some of the weights of a single neuron. If this happened, that individual neuron when passed on to the child, could be considered "{}mutated"{}, as we are not passing on a whole neuron, only a part of it. 

Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00201}{201}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.

\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_acb77c7337cf1440793b45b312aa2dac4}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!create@{create}}
\index{create@{create}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{create()}{create()}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_acb77c7337cf1440793b45b312aa2dac4} 
void DCL\+::\+CNeural\+Network\+Feed\+Forward\+::create (\begin{DoxyParamCaption}\item[{int}]{i\+Num\+Inputs}{, }\item[{int}]{i\+Num\+Outputs}{, }\item[{int}]{i\+Num\+Layers}{, }\item[{int}]{i\+Num\+Neurons\+Per\+Layer}{}\end{DoxyParamCaption})}



Creates/recreates the network. 


\begin{DoxyParams}{Parameters}
{\em i\+Num\+Inputs} & The number of inputs this network will have. \\
\hline
{\em i\+Num\+Outputs} & The number of outputs this network will have. \\
\hline
{\em i\+Num\+Layers} & The number of neuron layers this network will have. \\
\hline
{\em i\+Num\+Neurons\+Per\+Layer} & The number of neurons per layer this network will have. \\
\hline
\end{DoxyParams}


Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00037}{37}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.

\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_a9082844ff1fbbb3890d468c4acf3bd15}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!getNeuronWeights@{getNeuronWeights}}
\index{getNeuronWeights@{getNeuronWeights}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{getNeuronWeights()}{getNeuronWeights()}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_a9082844ff1fbbb3890d468c4acf3bd15} 
std\+::vector$<$ double $>$ DCL\+::\+CNeural\+Network\+Feed\+Forward\+::get\+Neuron\+Weights (\begin{DoxyParamCaption}\item[{void}]{}{}\end{DoxyParamCaption}) const}



Returns a vector holding all the weights of all the neurons in each of the layers, including the output layer. 

\begin{DoxyReturn}{Returns}
A vector holding all the weights of this network. 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00083}{83}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.

\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_af9100130b7f3c95cb02d9d59515388fa}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!getNumberOfWeights@{getNumberOfWeights}}
\index{getNumberOfWeights@{getNumberOfWeights}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{getNumberOfWeights()}{getNumberOfWeights()}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_af9100130b7f3c95cb02d9d59515388fa} 
int DCL\+::\+CNeural\+Network\+Feed\+Forward\+::get\+Number\+Of\+Weights (\begin{DoxyParamCaption}\item[{void}]{}{}\end{DoxyParamCaption}) const}



Returns the number of weights of all the neurons in each of the layers, including the output layer. 

\begin{DoxyReturn}{Returns}
The number of weights in this network. 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00103}{103}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.

\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_a5c907e87498ba77d90844fce32e4375d}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!replaceWeights@{replaceWeights}}
\index{replaceWeights@{replaceWeights}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{replaceWeights()}{replaceWeights()}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_a5c907e87498ba77d90844fce32e4375d} 
void DCL\+::\+CNeural\+Network\+Feed\+Forward\+::replace\+Weights (\begin{DoxyParamCaption}\item[{const std\+::vector$<$ double $>$ \&}]{new\+Weights}{}\end{DoxyParamCaption})}



Replaces the weights of all the neurons in each of the layers with the ones given. 


\begin{DoxyParams}{Parameters}
{\em new\+Weights} & A vector of doubles which hold the values of the new weights. \\
\hline
\end{DoxyParams}


Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00122}{122}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.

\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_a22e7d0f6ac35cad6149524a4db5971f1}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!setSigmoidResponse@{setSigmoidResponse}}
\index{setSigmoidResponse@{setSigmoidResponse}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{setSigmoidResponse()}{setSigmoidResponse()}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_a22e7d0f6ac35cad6149524a4db5971f1} 
void DCL\+::\+CNeural\+Network\+Feed\+Forward\+::set\+Sigmoid\+Response (\begin{DoxyParamCaption}\item[{double}]{d\+Response}{ = {\ttfamily 1}}\end{DoxyParamCaption})}



Sets the response value used by the sigmoid function which sets the shape of the curve produced from the sigmoid function. 


\begin{DoxyParams}{Parameters}
{\em d\+Response} & The response value used by the sigmoid function which sets the shape of the curve produced from the sigmoid function.\\
\hline
\end{DoxyParams}
Higher values, flatten the curve, lower ones tighten it. A default value of 1 is usually used. Do not set it to zero, this\textquotesingle{}ll create a divide by zero error. (Actually, an exception occurs) 

Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00195}{195}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.

\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_a28acb02d9185de77b5f480ac074622f2}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!setWeightBias@{setWeightBias}}
\index{setWeightBias@{setWeightBias}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{setWeightBias()}{setWeightBias()}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_a28acb02d9185de77b5f480ac074622f2} 
void DCL\+::\+CNeural\+Network\+Feed\+Forward\+::set\+Weight\+Bias (\begin{DoxyParamCaption}\item[{double}]{d\+Weight\+Bias}{ = {\ttfamily -\/1}}\end{DoxyParamCaption})}



Sets the bias which is used during update when computing the output for a neuron (weight \texorpdfstring{$\ast$}{*} input) \texorpdfstring{$\ast$}{*} bias. 


\begin{DoxyParams}{Parameters}
{\em d\+Weight\+Bias} & The bias used during updating the network and computing the output for each of it\textquotesingle{}s neurons (weight \texorpdfstring{$\ast$}{*} input) \texorpdfstring{$\ast$}{*} bias.\\
\hline
\end{DoxyParams}
This is typically set to -\/1 and left alone. 

Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00190}{190}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.

\Hypertarget{class_d_c_l_1_1_c_neural_network_feed_forward_a703b5d1758980e9365bd3be29525dd68}\index{DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}!update@{update}}
\index{update@{update}!DCL::CNeuralNetworkFeedForward@{DCL::CNeuralNetworkFeedForward}}
\doxysubsubsection{\texorpdfstring{update()}{update()}}
{\footnotesize\ttfamily \label{class_d_c_l_1_1_c_neural_network_feed_forward_a703b5d1758980e9365bd3be29525dd68} 
std\+::vector$<$ double $>$ DCL\+::\+CNeural\+Network\+Feed\+Forward\+::update (\begin{DoxyParamCaption}\item[{std\+::vector$<$ double $>$ \&}]{vec\+Inputs}{}\end{DoxyParamCaption})}



Given a vector of inputs, updates the neural network and returns the output values. 


\begin{DoxyParams}{Parameters}
{\em vec\+Inputs} & A vector holding each of the input values for the network. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A vector of doubles holding the output values of the network.
\end{DoxyReturn}
If invalid number of inputs is given, an exception occurs 

Definition at line \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source_l00141}{141}} of file \mbox{\hyperlink{_neural_net_feed_forward_8cpp_source}{Neural\+Net\+Feed\+Forward.\+cpp}}.



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
X\+:/\+Projects/c++/2024/\+DCP/\+Project\+Files/\+Static\+Libs/\+Daves\+Code\+Lib/\+Artificial\+Intelligence/\mbox{\hyperlink{_neural_net_feed_forward_8h}{Neural\+Net\+Feed\+Forward.\+h}}\item 
X\+:/\+Projects/c++/2024/\+DCP/\+Project\+Files/\+Static\+Libs/\+Daves\+Code\+Lib/\+Artificial\+Intelligence/Neural\+Net\+Feed\+Forward.\+cpp\end{DoxyCompactItemize}
